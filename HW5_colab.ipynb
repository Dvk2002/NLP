{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dvk2002/NLP/blob/main/HW5_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkpcHsV8RWHA"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAQBOJRARev7"
      },
      "source": [
        "**Написать теггер на данных с руским языком**\n",
        "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
        "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
        "3. сравнить все реализованные методы сделать выводы\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_16J0ER8WOJx"
      },
      "source": [
        "## загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPRx8Cu_RDY1",
        "outputId": "2b5dc9df-d322-4b2c-8c1b-778b7d11fa08"
      },
      "source": [
        "!pip install pyconll"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wgL-33mWUyZ"
      },
      "source": [
        "import pyconll"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXxwW9NzW570"
      },
      "source": [
        "!mkdir datasets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpwgA3svWiRw",
        "outputId": "ad2799c5-5cb3-4801-a1ba-8638619776a7"
      },
      "source": [
        "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
        "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-22 12:26:41--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81039282 (77M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-train.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  77.28M   104MB/s    in 0.7s    \n",
            "\n",
            "2021-07-22 12:26:42 (104 MB/s) - ‘./datasets/ru_syntagrus-ud-train.conllu’ saved [81039282/81039282]\n",
            "\n",
            "--2021-07-22 12:26:42--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10902738 (10M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  10.40M  37.4MB/s    in 0.3s    \n",
            "\n",
            "2021-07-22 12:26:43 (37.4 MB/s) - ‘./datasets/ru_syntagrus-ud-dev.conllu’ saved [10902738/10902738]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oymo30RBWjjl"
      },
      "source": [
        "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB8sWzKuf-R_"
      },
      "source": [
        "fdata_train = []\n",
        "\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])\n",
        "\n",
        "test_sent=fdata_sent_test[0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtjQ99oN3JMi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8FalvyXb9Qa"
      },
      "source": [
        "import nltk\n",
        "from nltk.tag import DefaultTagger\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import BigramTagger, TrigramTagger,ClassifierBasedTagger"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "IleaahU-0bK-",
        "outputId": "d8672357-7ddb-44c8-f533-a5985b7eb7eb"
      },
      "source": [
        "unigram_tagger = UnigramTagger(fdata_train)\n",
        "display(unigram_tagger.tag(test_sent), unigram_tagger.evaluate(fdata_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN'),\n",
              " ('аль', 'PART'),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', 'NOUN'),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', None),\n",
              " ('время', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8772537323492737"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "dj4tV8ytXTry",
        "outputId": "28e1924c-8f2a-4688-caf7-0ef4903698a2"
      },
      "source": [
        "bigram_tagger = BigramTagger(fdata_train)\n",
        "display(bigram_tagger.tag(test_sent), bigram_tagger.evaluate(fdata_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', None),\n",
              " ('аль', None),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', None),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', None),\n",
              " ('время', None),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.6963064064974893"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "4h1mtxEWmhsQ",
        "outputId": "756f934c-19ee-49dd-de7e-d0b5a8803ad7"
      },
      "source": [
        "trigram_tagger = TrigramTagger(fdata_train)\n",
        "display(trigram_tagger.tag(test_sent), trigram_tagger.evaluate(fdata_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', None),\n",
              " ('от', None),\n",
              " ('имени', None),\n",
              " ('учёного', None),\n",
              " ('аль', None),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', None),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', None),\n",
              " ('набор', None),\n",
              " ('инструкций', None),\n",
              " (',', None),\n",
              " ('описывающих', None),\n",
              " ('порядок', None),\n",
              " ('действий', None),\n",
              " ('исполнителя', None),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', None),\n",
              " ('результата', None),\n",
              " ('решения', None),\n",
              " ('задачи', None),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', None),\n",
              " ('время', None),\n",
              " ('.', None)]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.24808748694099012"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlWBW0KmzZiU",
        "outputId": "f66fe0f7-ca5d-455f-83eb-012879644037"
      },
      "source": [
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff\n",
        "\n",
        "\n",
        "backoff = DefaultTagger('NN') \n",
        "tag = backoff_tagger(fdata_train,  \n",
        "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
        "                     backoff = backoff) \n",
        "  \n",
        "tag.evaluate(fdata_test) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8814747413473528"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ_9dsBPc4DQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhEYEM5Xc4Ok",
        "outputId": "c7e511e5-ddf9-4d43-dcd9-c5105f323116"
      },
      "source": [
        "from nltk.tag import SequentialBackoffTagger\n",
        "nltk.download('names')\n",
        "from nltk.corpus import names\n",
        "\n",
        "\n",
        "class NamesTagger(SequentialBackoffTagger):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
        "        self.name_set = set([n.lower() for n in names.words()])\n",
        "            \n",
        "    def choose_tag(self, tokens, index, history):\n",
        "        word = tokens[index]\n",
        "        if 'star' in word.lower() :\n",
        "             return '★'\n",
        "        else:\n",
        "             return None\n",
        "            \n",
        "nt = NamesTagger()\n",
        "print(nt.tag(['SuperStar'])) \n",
        "print(nt.tag(['Starline'])) \n",
        "print(nt.tag(['Window']))            "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "[('SuperStar', '★')]\n",
            "[('Starline', '★')]\n",
            "[('Window', None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-_GkD1Cc4Ql"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1Q1iR7qWhhV"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Haw4Sr_Qn1n7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXO6225K989g"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_list=[]\n",
        "test_list=[]\n",
        "\n",
        "for token in fdata_test:\n",
        "  # print(token)\n",
        "  for word in token:\n",
        "    # print(word)\n",
        "    test_list.append(word)\n",
        "\n",
        "\n",
        "for token in fdata_train:\n",
        "  for word in token:\n",
        "    train_list.append(word)\n",
        "\n",
        "train_data=pd.DataFrame(train_list, columns=['word','tag'])\n",
        "test_data=pd.DataFrame(test_list, columns=['word','tag'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIXKCWLz2yjA"
      },
      "source": [
        "train = train_data[['word']]\n",
        "test= test_data[['word']]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bABLhwQLbdQj"
      },
      "source": [
        "train_data.tag=train_data.tag.fillna('None')\n",
        "test_data.tag=test_data.tag.fillna('None')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rymKY3oTkTj"
      },
      "source": [
        "le = LabelEncoder()\n",
        "train_enc = le.fit_transform(train_data['tag'].tolist())\n",
        "test_enc = le.transform(test_data['tag'].tolist())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh-ZEFR0n1rl"
      },
      "source": [
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV4Zvq_on1vC"
      },
      "source": [
        "vectorizers={'hash_v':HashingVectorizer(n_features=300),'tf_v': TfidfVectorizer(max_features=300),'count_v':CountVectorizer(max_features=300)}\n",
        "methods=['char','char_wb']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5Dt5NwHn1xQ",
        "outputId": "51f6656a-9857-4947-d0d2-2cf60a255f38"
      },
      "source": [
        "for name, vec  in vectorizers.items():\n",
        "  for meth in methods:\n",
        "    vectorizer =  vec.set_params(ngram_range=(2, 5), analyzer=meth)\n",
        "    train= vectorizer.fit_transform(train_data['word'].tolist()).toarray()\n",
        "    test = vectorizer.transform(test_data['word'].tolist()).toarray()\n",
        "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
        "    lr.fit(train, train_enc)\n",
        "    pred = lr.predict(test)\n",
        "    print(name,meth, accuracy_score(test_enc,pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash_v char 0.6821942506655883\n",
            "hash_v char_wb 0.8165841000235905\n",
            "tf_v char 0.7571024163380852\n",
            "tf_v char_wb 0.8493832777272268\n",
            "count_v char 0.7574225727091969\n",
            "count_v char_wb 0.8531156270009773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4RwRM5KYee7"
      },
      "source": [
        "Символьно-словесная векторизация дает лучшие результаты.\n",
        "CountVectorizer показывает лучшие результаты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHsXJWcLZFf3"
      },
      "source": [
        "Проверим работу модели на основе \"буста\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uwFL9I0ZHoD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR4UVPMhLgkt",
        "outputId": "58ab000f-328a-4536-9aee-d4872540afc2"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-0.26-cp37-none-manylinux1_x86_64.whl (69.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 69.2 MB 7.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2a4sZjIHI-Z"
      },
      "source": [
        "from catboost import CatBoostClassifier"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us6p7vyDLqCU"
      },
      "source": [
        "cb_params = {\n",
        "    \"n_estimators\": 30,\n",
        "    \"eval_metric\": \"Accuracy\",\n",
        "    \"task_type\": \"CPU\",\n",
        "    \"max_bin\": 20,\n",
        "    \"verbose\": 10,\n",
        "    \"max_depth\": 6,\n",
        "    'cat_features': ['word'],\n",
        "    # \"l2_leaf_reg\": 100,\n",
        "    \"early_stopping_rounds\": 20,\n",
        "    # \"thread_count\": 4,\n",
        "    \"random_seed\": 42\n",
        "}\n",
        "cb = CatBoostClassifier(**cb_params)\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdnEES0kM05q",
        "outputId": "3694507b-9618-4cdc-d661-363f1d59b848"
      },
      "source": [
        "cb.fit(train, train_enc,eval_set=[(test, test_enc)] )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate set to 0.471925\n",
            "0:\tlearn: 0.7508795\ttest: 0.7642133\tbest: 0.7642133 (0)\ttotal: 11.9s\tremaining: 5m 46s\n",
            "10:\tlearn: 0.8933342\ttest: 0.9066828\tbest: 0.9066913 (8)\ttotal: 1m 59s\tremaining: 3m 25s\n",
            "20:\tlearn: 0.8941913\ttest: 0.9065396\tbest: 0.9067839 (16)\ttotal: 3m 45s\tremaining: 1m 36s\n",
            "29:\tlearn: 0.8944059\ttest: 0.9067418\tbest: 0.9067839 (16)\ttotal: 5m 20s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.906783945\n",
            "bestIteration = 16\n",
            "\n",
            "Shrink model to first 17 iterations.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7f8a29eda8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPifVAFaYyzG"
      },
      "source": [
        "Результаты ожидаемо лучше, но разрыв с лучшими вариантами логистической регрессии не слишком велик.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blPr2YhX7DCy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cINqgGpKXURp"
      },
      "source": [
        "# Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCM0drjKXYet"
      },
      "source": [
        "много дополнительных датасетов на русском языке\n",
        "\n",
        "https://natasha.github.io/corus/  \n",
        "https://github.com/natasha/corus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUOg4C8sZNpw"
      },
      "source": [
        "мы будем использовать данные http://www.labinform.ru/pub/named_entities/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzi6ApNLZg6X"
      },
      "source": [
        "**Проверить насколько хорошо работает NER**\n",
        "\n",
        "1. взять нер из nltk\n",
        "2. проверить deeppavlov\n",
        "3. написать свой нер попробовать разные подходы:\n",
        "* передаём в сетку токен и его соседей\n",
        "* передаём в сетку только токен\n",
        "\n",
        "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1LgaNUtaOz"
      },
      "source": [
        "при обучении своего нера незабудьте разделить выборку"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg6tcss2Zhp9",
        "outputId": "d1711907-6ea9-4515-8020-f605e1b53f5b"
      },
      "source": [
        "!pip install corus"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting corus\n",
            "  Downloading corus-0.9.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: corus\n",
            "Successfully installed corus-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrc5ocDkaS1e"
      },
      "source": [
        "import corus"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPVv6lhT3ftA",
        "outputId": "6d7c9c82-da2e-4d8e-c9e3-37032c5cc3fd"
      },
      "source": [
        "!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-22 13:34:20--  http://www.labinform.ru/pub/named_entities/collection5.zip\n",
            "Resolving www.labinform.ru (www.labinform.ru)... 80.240.100.4\n",
            "Connecting to www.labinform.ru (www.labinform.ru)|80.240.100.4|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1899530 (1.8M) [application/zip]\n",
            "Saving to: ‘collection5.zip’\n",
            "\n",
            "collection5.zip     100%[===================>]   1.81M   445KB/s    in 4.2s    \n",
            "\n",
            "2021-07-22 13:34:25 (445 KB/s) - ‘collection5.zip’ saved [1899530/1899530]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtLK4z9CH2Ph"
      },
      "source": [
        "# !unzip collection5.zip"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEdS2pAS3fod",
        "outputId": "67fd3142-83d1-4ba0-dcba-4d1dbcd497e1"
      },
      "source": [
        "from corus import load_ne5\n",
        "\n",
        "dir = 'Collection5/'\n",
        "records = load_ne5(dir)\n",
        "next(records)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ne5Markup(\n",
              "    id='322',\n",
              "    text='К.Лагард приступила к работе на посту главы МВФ\\r\\n\\r\\nФранцуженка Кристин Лагард, избранная 28 июня новым директором - распорядителем МВФ, с сегодняшнего дня официально приступила к работе. Она стала первой женщиной в истории, возглавившей фонд. До этого К.Лагард занимала должность министра финансов Франции.\\r\\n\\r\\n55-летняя К.Лагард назначена главой МВФ на пятилетний срок. Выборы нового руководителя фонда понадобились после того, как в мае прежний управляющий фонда Доминик Стросс-Кан (также француз) был обвинен в попытке изнасилования горничной нью-йоркского отеля и ушел в отставку в связи с уголовным преследованием.\\r\\n\\r\\nСоперником К.Лагард на выборах главы МВФ был управляющий Центральным банком Мексики Агустин Карстенс, но ему не удалось преодолеть негласную конвенцию, согласно которой МВФ всегда руководит европеец.\\r\\n\\r\\nКак подчеркивают наблюдатели, К.Лагард возглавила МВФ в тот момент, когда фонд активно участвует в решении долговых проблем Греции и еврозоны в целом. Француженка хорошо осведомлена об этих проблемах, поскольку занималась ими на посту министра финансов своей страны.\\r\\n\\r\\nОднако теперь ей предстоит выступать от имени международной организации, которая формально представляет 187 стран и в последнее время активно критикуется за несправедливый \"перекос\" в пользу развитых западных государств.\\r\\n\\r\\n',\n",
              "    spans=[Ne5Span(\n",
              "         index='T1',\n",
              "         type='PER',\n",
              "         start=0,\n",
              "         stop=8,\n",
              "         text='К.Лагард'\n",
              "     ), Ne5Span(\n",
              "         index='T2',\n",
              "         type='ORG',\n",
              "         start=44,\n",
              "         stop=47,\n",
              "         text='МВФ'\n",
              "     ), Ne5Span(\n",
              "         index='T3',\n",
              "         type='PER',\n",
              "         start=63,\n",
              "         stop=77,\n",
              "         text='Кристин Лагард'\n",
              "     ), Ne5Span(\n",
              "         index='T4',\n",
              "         type='ORG',\n",
              "         start=131,\n",
              "         stop=134,\n",
              "         text='МВФ'\n",
              "     ), Ne5Span(\n",
              "         index='T5',\n",
              "         type='PER',\n",
              "         start=252,\n",
              "         stop=260,\n",
              "         text='К.Лагард'\n",
              "     ), Ne5Span(\n",
              "         index='T6',\n",
              "         type='GEOPOLIT',\n",
              "         start=298,\n",
              "         stop=305,\n",
              "         text='Франции'\n",
              "     ), Ne5Span(\n",
              "         index='T7',\n",
              "         type='PER',\n",
              "         start=320,\n",
              "         stop=328,\n",
              "         text='К.Лагард'\n",
              "     ), Ne5Span(\n",
              "         index='T8',\n",
              "         type='ORG',\n",
              "         start=346,\n",
              "         stop=349,\n",
              "         text='МВФ'\n",
              "     ), Ne5Span(\n",
              "         index='T9',\n",
              "         type='PER',\n",
              "         start=464,\n",
              "         stop=482,\n",
              "         text='Доминик Стросс-Кан'\n",
              "     ), Ne5Span(\n",
              "         index='T10',\n",
              "         type='PER',\n",
              "         start=633,\n",
              "         stop=641,\n",
              "         text='К.Лагард'\n",
              "     ), Ne5Span(\n",
              "         index='T11',\n",
              "         type='ORG',\n",
              "         start=659,\n",
              "         stop=662,\n",
              "         text='МВФ'\n",
              "     ), Ne5Span(\n",
              "         index='T12',\n",
              "         type='ORG',\n",
              "         start=679,\n",
              "         stop=697,\n",
              "         text='Центральным банком'\n",
              "     ), Ne5Span(\n",
              "         index='T13',\n",
              "         type='GEOPOLIT',\n",
              "         start=698,\n",
              "         stop=705,\n",
              "         text='Мексики'\n",
              "     ), Ne5Span(\n",
              "         index='T14',\n",
              "         type='PER',\n",
              "         start=706,\n",
              "         stop=722,\n",
              "         text='Агустин Карстенс'\n",
              "     ), Ne5Span(\n",
              "         index='T15',\n",
              "         type='ORG',\n",
              "         start=791,\n",
              "         stop=794,\n",
              "         text='МВФ'\n",
              "     ), Ne5Span(\n",
              "         index='T16',\n",
              "         type='PER',\n",
              "         start=855,\n",
              "         stop=863,\n",
              "         text='К.Лагард'\n",
              "     ), Ne5Span(\n",
              "         index='T17',\n",
              "         type='ORG',\n",
              "         start=875,\n",
              "         stop=878,\n",
              "         text='МВФ'\n",
              "     ), Ne5Span(\n",
              "         index='T18',\n",
              "         type='GEOPOLIT',\n",
              "         start=949,\n",
              "         stop=955,\n",
              "         text='Греции'\n",
              "     )]\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrhLNgNwQP2P"
      },
      "source": [
        "процедуры обработки взять из вебинарного ноутбука"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "sBTASf9b88mv",
        "outputId": "2df630e0-bf6e-4710-923a-0238756aefcd"
      },
      "source": [
        "next(records).text"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Главой Bundesbank станет советник канцлера ФРС по экономическим вопросам Й.Вайдман\\r\\n\\r\\nВместо Акселя Вебера главой Bundesbank (ЦБ ФРГ) станет советник канцлера Германии Ангелы Меркель по экономическим вопросам Йенс Вайдман. Об этом сообщает Reuters со ссылкой на источники в правительственных кругах страны. Й.Вайдман приступит к выполнению своих новых обязанностей в апреле 2011г., когда А.Вебер покинет пост главы ЦБ ФРГ. До этого 42-летний Й.Вайдман руководил отделом экономики в ведомстве федерального канцлера А.Меркель.\\r\\n\\r\\nПост вице-президента Bundesbank займет представитель Федерального управления финансового надзора Германии Сабина Лаутеншлегер, добавили высокопоставленные лица в Берлине.\\r\\n\\r\\nНапомним, 11 февраля 2011г. представитель правительственных кругов Германии Штефан Зайберт, сославшись на итоги встречи А.Вебера и канцлера ФРГ А.Меркель, сообщил, что глава Bundesbank А.Вебер покинет свой пост 30 апреля 2011г. По словам Ш.Зайберта, А.Вебер оставляет эту должность по \"личным обстоятельствам\".\\r\\n\\r\\nДо недавнего времени А.Вебер считался одним из наиболее вероятных кандидатов на пост главы Европейского центробанка (ЕЦБ) после истечения в ноябре 2011г. срока полномочий действующего председателя банка Жан-Клода Трише. Позже в прессу просочилась информация о том, что А.Вебер не претендует ни на пост главы ЕЦБ, ни на пост главы ЦБ ФРГ на новый срок. Сам Ж.-К.Трише отказался комментировать слухи о возможной кандидатуре А.Вебера.\\r\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnvYd4x4yPAP"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRuODJpkIqlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af14cde2-ab7b-41f5-bac7-57851bc0b5d0"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDdnL6EXJRt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3061817-3180-4f6d-cd79-758c4ae5d90c"
      },
      "source": [
        "document=next(records).text\n",
        "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('Allyson Campa', 'PERSON'),\n",
              " ('Anne Gillespie', 'PERSON'),\n",
              " ('Christopher Dean', 'PERSON'),\n",
              " ('David Gurle', 'PERSON'),\n",
              " ('Don Albert', 'ORGANIZATION'),\n",
              " ('Doug Bewsher', 'PERSON'),\n",
              " ('FTC', 'ORGANIZATION'),\n",
              " ('Microsoft', 'ORGANIZATION'),\n",
              " ('Microsoft', 'PERSON'),\n",
              " ('Ramu Sunkara', 'ORGANIZATION'),\n",
              " ('Russ Shaw', 'ORGANIZATION'),\n",
              " ('Skype', 'PERSON'),\n",
              " ('Skype Microsoft', 'PERSON'),\n",
              " ('Skype Тони Бейтс', 'PERSON'),\n",
              " ('Tony Bates', 'PERSON'),\n",
              " ('Дон Альберт', 'PERSON'),\n",
              " ('Источник Bloomberg', 'PERSON'),\n",
              " ('Компания Skype', 'ORGANIZATION'),\n",
              " ('Кристофер Дин', 'PERSON'),\n",
              " ('Кроме', 'PERSON'),\n",
              " ('Предполагается', 'PERSON'),\n",
              " ('Расс Шоу', 'PERSON'),\n",
              " ('Сервис Qik Skype', 'PERSON'),\n",
              " ('Эллисон Кампа', 'PERSON'),\n",
              " ('Энн Гиллеспи', 'PERSON')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWcBEhNI8KO7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kd-emBao1u-"
      },
      "source": [
        "установка deeppavlov\n",
        "\n",
        "!pip uninstall -y tensorflow tensorflow-gpu\n",
        "!pip install numpy scipy librosa unidecode inflect librosa transformers\n",
        "!pip install deeppavlov"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY96lqBzsZJ_"
      },
      "source": [
        "!python -m deeppavlov install squad_bert\n",
        "!python -m deeppavlov install ner_ontonotes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KE7tpVWs1b7"
      },
      "source": [
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsegbgCbrzy_",
        "outputId": "4146e92e-dc03-4af9-e305-decd325247d0"
      },
      "source": [
        "deeppavlov_ner = build_model(configs.ner.ner_bert_ent_and_type_rus, download=True)\n",
        "rus_document = \"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\n",
        "deeppavlov_ner([rus_document])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-18 22:12:05.88 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/kbqa/datasets/entity_and_type_detection_rus.pickle to /root/.deeppavlov/models/entity_and_type_detection_rus.pickle\n",
            "100%|██████████| 2.07M/2.07M [00:00<00:00, 2.39MB/s]\n",
            "2021-07-18 22:12:07.621 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip to /root/.deeppavlov/downloads/multi_cased_L-12_H-768_A-12.zip\n",
            "100%|██████████| 663M/663M [01:46<00:00, 6.24MB/s]\n",
            "2021-07-18 22:13:54.691 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/downloads/multi_cased_L-12_H-768_A-12.zip archive into /root/.deeppavlov/downloads/bert_models\n",
            "2021-07-18 22:14:02.788 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/kbqa/models/ner_cq_rus.tar.gz to /root/.deeppavlov/models/ner_cq_rus.tar.gz\n",
            "100%|██████████| 1.32G/1.32G [05:40<00:00, 3.87MB/s]\n",
            "2021-07-18 22:19:44.325 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/models/ner_cq_rus.tar.gz archive into /root/.deeppavlov/models/ner_ent_and_type_rus\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-18 22:20:04.397 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_ent_and_type_rus/tag.dict]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:571: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:94: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:671: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-18 22:20:36.925 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/ner_ent_and_type_rus/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/ner_ent_and_type_rus/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['Нью',\n",
              "   '-',\n",
              "   'Йорк',\n",
              "   ',',\n",
              "   'США',\n",
              "   ',',\n",
              "   '30',\n",
              "   'апреля',\n",
              "   '2020',\n",
              "   ',',\n",
              "   '01',\n",
              "   ':',\n",
              "   '01',\n",
              "   '—',\n",
              "   'REGNUM',\n",
              "   'В',\n",
              "   'администрации',\n",
              "   'президента',\n",
              "   'США',\n",
              "   'Дональда',\n",
              "   'Трампа',\n",
              "   'планируют',\n",
              "   'пройти',\n",
              "   'все',\n",
              "   'этапы',\n",
              "   'создания',\n",
              "   'вакцины',\n",
              "   'от',\n",
              "   'коронавируса',\n",
              "   'в',\n",
              "   'ускоренном',\n",
              "   'темпе',\n",
              "   'и',\n",
              "   'выпустить',\n",
              "   '100',\n",
              "   'млн',\n",
              "   'доз',\n",
              "   'до',\n",
              "   'конца',\n",
              "   '2020',\n",
              "   'года',\n",
              "   ',',\n",
              "   'передаёт',\n",
              "   'агентство',\n",
              "   'Bloomberg',\n",
              "   'со',\n",
              "   'ссылкой',\n",
              "   'на',\n",
              "   'осведомлённые',\n",
              "   'источники']],\n",
              " array([[[9.71761167e-01, 2.64181718e-02, 1.82059826e-03],\n",
              "         [9.60046053e-01, 3.84359621e-02, 1.51804090e-03],\n",
              "         [9.21502233e-01, 7.51483515e-02, 3.34933866e-03],\n",
              "         [9.92245018e-01, 6.89126039e-03, 8.63717811e-04],\n",
              "         [9.87261832e-01, 1.17831351e-02, 9.55077179e-04],\n",
              "         [9.96919036e-01, 2.74484046e-03, 3.36058962e-04],\n",
              "         [9.97517467e-01, 2.30196840e-03, 1.80645322e-04],\n",
              "         [9.95577574e-01, 4.12209751e-03, 3.00363230e-04],\n",
              "         [9.91355956e-01, 8.05664156e-03, 5.87437069e-04],\n",
              "         [9.98594940e-01, 1.27561530e-03, 1.29398570e-04],\n",
              "         [9.98789489e-01, 1.13862741e-03, 7.19211675e-05],\n",
              "         [9.97291386e-01, 2.53513828e-03, 1.73508219e-04],\n",
              "         [9.98374701e-01, 1.53756991e-03, 8.77227576e-05],\n",
              "         [9.97647226e-01, 2.15254142e-03, 2.00248949e-04],\n",
              "         [9.96125996e-01, 3.52644222e-03, 3.47553432e-04],\n",
              "         [9.98647392e-01, 1.18941162e-03, 1.63180783e-04],\n",
              "         [9.94467735e-01, 4.99971071e-03, 5.32494334e-04],\n",
              "         [9.88695621e-01, 1.04803387e-02, 8.23963608e-04],\n",
              "         [9.90075767e-01, 8.98968987e-03, 9.34486161e-04],\n",
              "         [1.60550028e-01, 8.38216543e-01, 1.23350031e-03],\n",
              "         [1.39164224e-01, 8.59113455e-01, 1.72229693e-03],\n",
              "         [9.98087585e-01, 1.73141155e-03, 1.80948962e-04],\n",
              "         [9.98838842e-01, 1.03886123e-03, 1.22317724e-04],\n",
              "         [9.98723567e-01, 1.15420937e-03, 1.22209996e-04],\n",
              "         [9.97251451e-01, 2.43649492e-03, 3.12116492e-04],\n",
              "         [9.96265590e-01, 3.32576130e-03, 4.08622727e-04],\n",
              "         [9.88883674e-01, 1.00314673e-02, 1.08485715e-03],\n",
              "         [9.81114209e-01, 1.81934275e-02, 6.92377449e-04],\n",
              "         [3.06576163e-01, 6.90057814e-01, 3.36598023e-03],\n",
              "         [9.96585727e-01, 3.16357287e-03, 2.50688347e-04],\n",
              "         [9.96187031e-01, 3.55201564e-03, 2.60923203e-04],\n",
              "         [9.94267046e-01, 5.20009315e-03, 5.32936596e-04],\n",
              "         [9.98957992e-01, 9.21283732e-04, 1.20748897e-04],\n",
              "         [9.98963594e-01, 9.19635117e-04, 1.16777301e-04],\n",
              "         [9.94871259e-01, 4.77636745e-03, 3.52344301e-04],\n",
              "         [9.92532551e-01, 6.92349765e-03, 5.44037437e-04],\n",
              "         [9.94569838e-01, 4.95819980e-03, 4.71973879e-04],\n",
              "         [9.98663902e-01, 1.20707951e-03, 1.29073000e-04],\n",
              "         [9.97741580e-01, 2.10796879e-03, 1.50471955e-04],\n",
              "         [9.95123923e-01, 4.60237823e-03, 2.73693877e-04],\n",
              "         [9.98261988e-01, 1.58725481e-03, 1.50823776e-04],\n",
              "         [9.98117447e-01, 1.66510267e-03, 2.17488472e-04],\n",
              "         [9.98956561e-01, 9.41986626e-04, 1.01515136e-04],\n",
              "         [9.94731545e-01, 4.65001678e-03, 6.18365128e-04],\n",
              "         [8.65644813e-01, 1.27655372e-01, 6.69975672e-03],\n",
              "         [9.98962879e-01, 9.61940445e-04, 7.51710177e-05],\n",
              "         [9.99106348e-01, 8.36298510e-04, 5.72500976e-05],\n",
              "         [9.98937190e-01, 9.87627311e-04, 7.51964762e-05],\n",
              "         [9.96844649e-01, 2.94215581e-03, 2.13187697e-04],\n",
              "         [9.94291425e-01, 5.16543631e-03, 5.43111411e-04]]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6vcgcN6DjRR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ1phPKisJMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85dff3e2-95a1-4ae6-8a21-478b9c199dba"
      },
      "source": [
        "!pip install razdel"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUrI8RBwDaWN"
      },
      "source": [
        "from razdel import tokenize"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anRNTpweD0NU"
      },
      "source": [
        "words_docs = []\n",
        "for ix, rec in enumerate(records):\n",
        "    words = []\n",
        "    for token in tokenize(rec.text):\n",
        "        # if \"http://\" in input:\n",
        "        #   token='None'\n",
        "        type_ent = 'OUT'\n",
        "        for ent in rec.spans:\n",
        "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
        "                type_ent = ent.type\n",
        "                break\n",
        "        words.append([token.text, type_ent])\n",
        "    words_docs.extend(words)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiS4-yZ68Keg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CXne5KID65Z"
      },
      "source": [
        "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1Km8ChsEAPy",
        "outputId": "8da1a560-b2b0-42ce-9814-e79d6703672b"
      },
      "source": [
        "df_words['tag'].value_counts()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OUT         218871\n",
              "PER          21122\n",
              "ORG          13625\n",
              "LOC           4568\n",
              "GEOPOLIT      4345\n",
              "MEDIA         2481\n",
              "Name: tag, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6of10x7n6JRs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCrglaAMECpz"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input, Bidirectional,Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPmhp7aDQzbS"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
        "\n",
        "# labelEncode целевую переменную\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEUPsz9yQz7E",
        "outputId": "a9bc18dd-37f9-4f85-a630-42e5ed180a40"
      },
      "source": [
        "train_x.apply(len).max(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDDCrhW4Q0Cn"
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
        "\n",
        "train_data = train_data.batch(2048)\n",
        "valid_data = valid_data.batch(2048)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lea5OyUsSVgt"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLGmY51FC5JW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKXkqZ0JbVoU"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    return input_data\n",
        "\n",
        "vocab_size = 30000\n",
        "seq_len = 10\n",
        "\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    # ngrams=(1, 3),\n",
        "    output_sequence_length=seq_len\n",
        "    )\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_data = train_data.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_data)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Z5VnGQnLoe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-fBPosQHYMZ",
        "outputId": "5d48e772-d4a9-46d6-d401-d04edd4b7753"
      },
      "source": [
        "t=np.unique(encoder.inverse_transform(valid_y),return_counts=True)[1]\n",
        "t=t/t.sum()\n",
        "t  # распределение таргета"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01645154, 0.01696423, 0.00943965, 0.05223475, 0.82606008,\n",
              "       0.07884975])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXf4pgQZh_An"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUN0z3vjtt6Q"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  tf.keras.layers.Embedding(len(vectorize_layer.get_vocabulary()), 64, mask_zero=True),\n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  tf.keras.layers.Dense(300, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(50, activation='relu'),\n",
        "  # tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(6, activation='softmax')\n",
        "  ])\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbRs-r6083gg",
        "outputId": "29f4fc43-e525-4967-a013-0b5fa974037e"
      },
      "source": [
        "model.compile(optimizer='adam',         \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_data, validation_data=valid_data, epochs=3)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "98/98 [==============================] - 6s 57ms/step - loss: 0.1311 - accuracy: 0.9613 - val_loss: 0.2216 - val_accuracy: 0.9418\n",
            "Epoch 2/3\n",
            "98/98 [==============================] - 5s 55ms/step - loss: 0.1141 - accuracy: 0.9638 - val_loss: 0.2216 - val_accuracy: 0.9422\n",
            "Epoch 3/3\n",
            "98/98 [==============================] - 5s 56ms/step - loss: 0.1086 - accuracy: 0.9644 - val_loss: 0.2186 - val_accuracy: 0.9425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc3fdaf0b90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXvB8xKktuA1"
      },
      "source": [
        "pred=model.predict(valid_data)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvV3FwBJ9jlw",
        "outputId": "4230d50f-750d-43df-b333-06eda46851f2"
      },
      "source": [
        "list(zip(encoder.classes_,pred.mean(axis=0)/t))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('GEOPOLIT', 0.9731310894533541),\n",
              " ('LOC', 0.8864983825220002),\n",
              " ('MEDIA', 1.016243024202534),\n",
              " ('ORG', 1.3284911842840375),\n",
              " ('OUT', 0.9950426883681494),\n",
              " ('PER', 0.8640988755121265)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUZK47ts9qHS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQoJiwsZiXYS"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    tf.keras.layers.Embedding(len(vectorize_layer.get_vocabulary()), 64, mask_zero=True),\n",
        "    # tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(6,activation='softmax')\n",
        "])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR3qMDVziXcV"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6prYtAgCiXfe",
        "outputId": "29817c76-6e1a-4379-d421-4c7576075766"
      },
      "source": [
        "mmodel.fit(train_data, validation_data=valid_data, epochs=3)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "98/98 [==============================] - 5s 46ms/step - loss: 0.1082 - accuracy: 0.9646 - val_loss: 0.2293 - val_accuracy: 0.9414\n",
            "Epoch 2/3\n",
            "98/98 [==============================] - 5s 47ms/step - loss: 0.1066 - accuracy: 0.9649 - val_loss: 0.2307 - val_accuracy: 0.9415\n",
            "Epoch 3/3\n",
            "98/98 [==============================] - 5s 46ms/step - loss: 0.1048 - accuracy: 0.9650 - val_loss: 0.2315 - val_accuracy: 0.9417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f181c9ebcd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmZe1D29c_Qv"
      },
      "source": [
        "\"Стационарная\" классификация дает примерно такие же результаты, как и \"динамическая\". Возможно, более глубокая предобработка текста и усложнение модели сможет изменить соотношение результатов."
      ]
    }
  ]
}